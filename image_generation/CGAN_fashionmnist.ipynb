{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf25e27-6e08-4ed0-813e-7b162f06cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88d93b94-8253-48c8-b9af-05c6edc4f4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='NVIDIA GeForce RTX 3060 Laptop GPU', major=8, minor=6, total_memory=6143MB, multi_processor_count=30)\n"
     ]
    }
   ],
   "source": [
    "device_count = torch.cuda.device_count()\n",
    "for i in range(device_count):\n",
    "    print(torch.cuda.get_device_properties(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14883704-ee89-469d-bc1c-9cb9a67c0b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNIST(Dataset):\n",
    "    def __init__(self, train=True, transform=False):\n",
    "        self.transform = None\n",
    "        if transform: \n",
    "            self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))  \n",
    "        ])\n",
    "        self.path: str = r'/ml_notebooks/'\n",
    "        \n",
    "        self.data = datasets.FashionMNIST(\n",
    "            root=self.path,\n",
    "            train=train,\n",
    "            download=True,\n",
    "            transform= self.transform  \n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.data[idx]  \n",
    "        print('shape',np.array(img).shape,'\\n')\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0978e4c-f446-40a7-9eae-9012413644bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = FashionMNIST(train=True, transform=False)\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bba7ef01-4c05-43c3-832c-1187b3e21196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (28, 28) \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+tbw1oNx4m8QWmkWx2yXD4LkZCADJJ+gFbviL4a63oc7COE3MW4hdn38duD976jNc9daDqllIsc9lKrMu4YGeMkdR7gj8KzcV7H8BtEvV16+1iWCeG1Wz8mOV02pIzupwCeuAp6Z98cZ90aIzLIlw0c0ZJ4KgjHoeOa+evjS9n/wnMcNxBPCYLKONFhA2FNzMpGenDcgd816V4K03wefC+m3NlpVhP+5QSXBiR5fMx825iMg5zwce3FdbOzTwgW90lu6uCm8eYrL02soIyCPQgggEdMGQ3cluiPNK0rJwrRQBNueuMkt+teNfGKxsdY8WWdxNqcNo66eieXMwVsb5DnH415Hp2rajpE5n02/urOUjBe3laMkehIPIrVm8eeLrhNknibVivoLtx/I1UPinxC3XXtUP1vJP8ay5JZJpGkldnduSzHJP41//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACD0lEQVR4AbWRz2sTQRTH38zszm42k6Q2TU2ixURstRdRasWC9FCsUE9SpIKnKl48+x94UOjNiyf/h4KgKN7TYgNKK+agjZS0wZDYbND90dn54VqxIWfxXebBB97nfecB/P9CAKkFAGT8ViEEh+9fLZZn7gde+E4AwkgAGYBEzl3btZz55y0tgSl/AHKYLhH85uJKdat2ebqyFmuOCun5laFIwcYXjvLRxq1nfRh3er0ESHAI1fvPYqF8oj9WxxO6hcAyWZhQV2fw6OvBbcEh2O/tlxTCjlRjgPtGAqwYcpN7GWUlPbppXepDTeB2AduQHONWpOzsi09GHxocPobUccZPh8RhcvfOy/V4IUQwigMIgFdeQHWb2BFEipzvxU6iBT9QALNPq8F3oYTnSRt8oN4iHOYcLk4UFs+GOEo0TZrlToXNqp7ZmkQw8yg3JIkrHI6C2lI1dawE9dQPP8HSDiJrRSF9IAFAZmT5+oNm+LU+nuVmispT6N6TbcYsMDONZg7nb9rl5NQU5pgCMq8YrUY6bDCa3t9hQShWt0rD3I0kNxWiE8aebiRH3E7bsEw7hTuTXqNrdSIRJfK9C8aH1bvNesioTcmB1P43JY2QcdeNRLkVR7nx8HjblYQaBGnTpCYC1AKq8ptLCMf55x6PZjAxJGrpPfWTgI58/LZW+fMJ8WXO5bond/j20Y3+sfkFaCTYdrBYeB0AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "289f7b18-9854-4a0a-bda2-b26545c87da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = FashionMNIST(train=False, transform=False)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aba6f10d-6b0c-435e-940e-7faccb83b2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (28, 28) \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iilIIOCCPrSVueHPD0us3YZ1ZbKM5lfpu/2V9Sf06162fBvh3UI0mu9LXz1iHKyMg4HC4UgHA4zXmHxA00aZ4tmjjjWO3kijeBV6bAoXj2BUj8K0tEvPBAgiW4E1tOqrve6tfOUv3IKnOM/7NdZB4o8I2yhn1uBgg+VLezm3H2AZQB+daWmfEXwPdLMl3f6pZBFXYZrUNvJznAQtjHHU81xvjjxXoeo61BJpErXFsluELvblDu3Mehx2Irzmiiiv/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABWElEQVR4Ad2Ru04CURCG/3POssteWDAbLmIiWhCixFAZjcSOaE9oLGyk8QF8LxMLK5/A0srERqOGSxSEZZFlz0UINvAExulmvvkz/8wA/zoYAQwUAbq0JqHYaNhCASHqqxASx9XmvD9z7gPakpLx/Z1O8bpnvnju24qScrtB4glCSTnqz2S/MwkoAZO47LTNeEfIYKq79gISKEjFBM5y7VHqsxtpMVC7uIAKVCMQuKi8eilzmJDjiCiczg1RKCIlkK+bT47hTZUFEYpAVqExMasrpLdK69Nhyo0ZshD7iiT9Zn5ZE8gWbNvctqIRTZrcGod6K2n1nbUg52mo5XmaCe47OWL0qcOC0SAD9KVpcO2k+djy6ZQRX5cJYspYLrurUwTWZNTV7g/3quB+rzfQiVeyXFV5eK4ZCvx9mJg9AM5B6Sjt2kTJ3uPd7QS42fzwfR5eLZ32zyc/YoJ9/8OxiXQAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eace2f1-8fb3-4f7d-8cbf-4be74ac748fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(10,10)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(794,1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        ) \n",
    "    def forward(self, x, labels):\n",
    "        x = x.view(x.size(0), 784) # 28*28 \n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([x,c], 1)\n",
    "        out = self.model(x)\n",
    "        return out.squeeze()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20c4d2ab-cea2-445c-8673-2124ce0c001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(10, 10) \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(110, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        z = z.view(z.size(0), 100)  \n",
    "        c = self.label_emb(labels)  \n",
    "        x = torch.cat([z, c], 1)  \n",
    "        out = self.model(x)  \n",
    "        return out.view(x.size(0), 28, 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e98aa4f3-48ac-4a76-a49c-db5fc0de58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator().cuda()\n",
    "disc = Discriminator().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4109d8b7-da3b-492c-b639-cae90df65000",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(disc.parameters(), lr=1e-4)\n",
    "g_optimizer = torch.optim.Adam(gen.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a21b1f3b-15be-413b-bac4-b3ff19714840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion):\n",
    "    g_optimizer.zero_grad()\n",
    "    z = Variable(torch.randn(batch_size, 100)).cuda()\n",
    "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, 10, batch_size))).cuda()\n",
    "    fake_images = generator(z, fake_labels)\n",
    "    validity = discriminator(fake_images, fake_labels)\n",
    "    g_loss = criterion(validity, Variable(torch.ones(batch_size)).cuda())\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    return g_loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc273e28-ad5d-4eba-89b7-754356fce05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, real_images, labels):\n",
    "    d_optimizer.zero_grad()\n",
    "\n",
    "    # train with real images\n",
    "    real_validity = discriminator(real_images, labels)\n",
    "    real_loss = criterion(real_validity, Variable(torch.ones(batch_size)).cuda())\n",
    "    \n",
    "    # train with fake images\n",
    "    z = Variable(torch.randn(batch_size, 100)).cuda()\n",
    "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, 10, batch_size))).cuda()\n",
    "    fake_images = generator(z, fake_labels)\n",
    "    fake_validity = discriminator(fake_images, fake_labels)\n",
    "    fake_loss = criterion(fake_validity, Variable(torch.zeros(batch_size)).cuda())\n",
    "    \n",
    "    d_loss = real_loss + fake_loss\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    return d_loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e0bf70d-5074-471f-9931-4e360880e663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0...\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n",
      "shape (1, 28, 28) \n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m     gen\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     12\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m real_images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     d_loss \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     g_loss \u001b[38;5;241m=\u001b[39m generator_train_step(batch_size, disc, gen, g_optimizer, criterion)\n\u001b[0;32m     20\u001b[0m generator\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[25], line 18\u001b[0m, in \u001b[0;36mdiscriminator_train_step\u001b[1;34m(batch_size, discriminator, generator, d_optimizer, criterion, real_images, labels)\u001b[0m\n\u001b[0;32m     16\u001b[0m d_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     17\u001b[0m d_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43md_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "n_critic = 5\n",
    "display_step = 300\n",
    "train_data = FashionMNIST(train=True, transform=True)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "for epoch in range(num_epochs):\n",
    "    print('Starting epoch {}...'.format(epoch))\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        real_images = Variable(images).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        gen.train()\n",
    "        batch_size = real_images.size(0)\n",
    "        d_loss = discriminator_train_step(len(real_images), disc,\n",
    "                                          gen, d_optimizer, criterion,\n",
    "                                          real_images, labels)\n",
    "        \n",
    "\n",
    "        g_loss = generator_train_step(batch_size, disc, gen, g_optimizer, criterion)\n",
    "\n",
    "    generator.eval()\n",
    "    print('g_loss: {}, d_loss: {}'.format(g_loss, d_loss))\n",
    "    z = Variable(torch.randn(9, 100)).cuda()\n",
    "    labels = Variable(torch.LongTensor(np.arange(9))).cuda()\n",
    "    sample_images = gen(z, labels).unsqueeze(1).data.cpu()\n",
    "    grid = make_grid(sample_images, nrow=3, normalize=True).permute(1,2,0).numpy()\n",
    "    plt.imshow(grid)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d26ef8-5f82-4f6f-ac20-54806a857983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf171024-env)",
   "language": "python",
   "name": "tf171024-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
